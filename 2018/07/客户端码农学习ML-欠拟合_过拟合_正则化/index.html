<!DOCTYPE html>
<html lang="zh-cmn-Hans">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>客户端码农学习ML —— 欠拟合_过拟合_正则化 | 钱凯凯</title>
  
  
  <!--link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css"-->
  <link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css">
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
<div class="Shell">
    <aside class='SideBar'>
    <section class='avatar' style="background-image: url()">
        <div class='av-pic' style="background-image: url(/assets/avatar.jpg)">
        </div>
    </section>
    <section class='menu'>
        <div>钱凯凯</div>
        
            <div>钱凯凯的博客</div>
        
        <ul>
          
            <a href="/" class="Btn">
              <li>Home</li>
            </a>  
          
            <a href="/archives/" class="Btn">
              <li>Archive</li>
            </a>  
          
        </ul>
    </section>
    <section class="media">
        
            
                <a href="https://github.com/qianhk">
                    <img src="/assets/github.svg" />
                </a>
            
        
    </section>
</aside>

    <div class="container">
        <div data-pager-shell>
            <div>
  <article class='ContentView'>
    <header class='PageTitle'>
        <h1>客户端码农学习ML —— 欠拟合_过拟合_正则化</h1>
    </header>

    <section>
      <p>在机器学习的训练中，欠拟合、过拟合是个绕不过去的问题，本文则试验下这两种现象，并对过拟合问题尝试使用正则化来避免，同时为了加快训练速度，对特征进行了缩放，使多个特征的值大小范围处于同一个量级，体验了一把特征缩放对训练速度的影响。</p>
<a id="more"></a>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>通过numpy库创建一个数组，作为只有一个特征的样本，同时通过log函数创建目标值数组，并加点随机偏移。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pie_size = np.array([-2, -1, 0, 1, 2], dtype=np.float32)</span><br><span class="line">pie_price = np.log((pie_size + 3) * 10)</span><br><span class="line">pie_price += np.random.normal(0, 0.3, [5])</span><br></pre></td></tr></table></figure>
<p>这个特征可以理解为饼的大小(1寸到5寸，为了后续计算平方、立方、高次方的数值小，防止NAN，有意减3)，价格在饼的大小基础上通过log方法得到。</p>
<p>后续试验假设我们并不知道这些样本的特征与标签的真正关系，而是通过增加多项式来拟合。</p>
<h2 id="欠拟合"><a href="#欠拟合" class="headerlink" title="欠拟合"></a>欠拟合</h2><p>通过学习我们知道欠拟合通常是模型过于简单，不能较好的拟合训练样本，因此通过单变量来尝试拟合试试效果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">b = tf.Variable(0, dtype=tf.float32)</span><br><span class="line">w1 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">learn_rate = 0.01</span><br><span class="line">target = b + w1 * pie_size</span><br><span class="line">loss = tf.reduce_mean(tf.square(target - pie_price))</span><br><span class="line"></span><br><span class="line"># 以下样本代码忽略</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learn_rate)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<p>熟悉线性回归的可以看出这其实就是个单变量线性回归，拟合出来是一条直线，显然不能拟合出像log的图形。</p>
<p><img src="/images/fitting_underfitting.jpg" alt="fitting_underfitting"></p>
<h2 id="拟合合适"><a href="#拟合合适" class="headerlink" title="拟合合适"></a>拟合合适</h2><p>相对于单变量的基础上，再增加一个变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w2 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"># 特征可以实现组合成平方、立方等形成新的特征，并改写成矩阵乘法更简洁</span><br><span class="line">target = b + w1 * pie_size + w2 * pie_size ** 2</span><br><span class="line">return tf.reduce_mean(tf.square(target - pie_price))</span><br></pre></td></tr></table></figure>
<p>从下图可以看出比较接近log函数的样子了。</p>
<p><img src="/images/fitting_fitting_ok.jpg" alt="fitting_fitting_ok"></p>
<h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p>为了模拟过拟合，再增加两个变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w3 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line">w4 = tf.Variable(0, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<h3 id="无特征缩放"><a href="#无特征缩放" class="headerlink" title="无特征缩放"></a>无特征缩放</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learn_rate = 0.0001</span><br><span class="line">target = b + w1 * pie_size + w2 * pie_size ** 2</span><br><span class="line">	 + w3 * pie_size ** 3 + w4 * pie_size ** 4</span><br><span class="line">return tf.reduce_mean(tf.abs(target - pie_price))</span><br></pre></td></tr></table></figure>
<h3 id="有特征缩放"><a href="#有特征缩放" class="headerlink" title="有特征缩放"></a>有特征缩放</h3><p>原始值范围是0-2， 二次方、三次方、四次方后分别最大4、8、16，所以我简单的分别除以2、4、8，将至缩放到同一量级，比较常见的算法是min-max算法，都缩放到0-1之间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learn_rate = 0.01</span><br><span class="line"># 同样，特征可以实现组合成平方、立方等多次方后再除以对应的缩放系数形成新的特征，并改写成矩阵乘法</span><br><span class="line">target = b + w1 * pie_size + w2 * pie_size ** 2 / 2</span><br><span class="line">	 + w3 * pie_size ** 3 / 4 + w4 * pie_size ** 4 / 8</span><br></pre></td></tr></table></figure>
<p>特征缩放后，可以提高学习率，在我2014款mbp上只要2、3秒就可以完美拟合这5个样本点。</p>
<p>而未进行特征缩放的时候，虽然我有意选择了较小的特征数值，但由于4次方后的值较大，学习率得降低，不然训练过程中不是NAN就是损失先降低又上升，总之无法拟合。较低的学习率使得训练时长大幅增加，得到相似的结果大概需要40秒。</p>
<p><img src="/images/fitting_overfitting.jpg" alt="fitting_overfitting"></p>
<p>即使不用测试集，从图中也明显看出过拟合了，虽然5个点都完美穿过，但这压根儿不像是log图形的样子，尤其是右边，本应向上却几乎直线下降了。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>对于本地我们可以通过减少特征的方法避免过拟合现象，对于样本少特征多且没有明显特征可以减少的的情况我们试试正则化方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">learn_rate = 0.001</span><br><span class="line"></span><br><span class="line">regularization_strength = 0.2</span><br><span class="line">regularization_result = regularization_strength \</span><br><span class="line">                        * (tf.abs(w1) + tf.abs(w2) + tf.abs(w3) + tf.abs(w4))</span><br><span class="line">target = b + w1 * pie_size + w2 * pie_size ** 2 / 2</span><br><span class="line">	 + w3 * pie_size ** 3 / 4 + w4 * pie_size ** 4 / 8</span><br><span class="line">loss = tf.reduce_mean(tf.abs(target - pie_price)) + regularization_result</span><br><span class="line"></span><br><span class="line"># 最终参数值为： bias=3.6952 w1=0.4110 w2=-0.0098 w3=-0.0000 w4=-0.0099</span><br></pre></td></tr></table></figure>
<p>本次通过L1正则化试验，可以看到w3几乎被降为了0，可以认为这个特征可以忽略掉了，由于增加正则化，试验下来相比单纯的仅特征缩放不用正则化，学习率要降低些，时间上有所增加。</p>
<p><img src="/images/fitting_overfitting_scale_l1.jpg" alt="fitting_overfitting_scale_l1"></p>
<p>可以通过调整正则化强度regularization_strength来控制拟合平滑程度。</p>
<p>从图形上看跟上面第二种有两个变量的情况下差不多，拟合较好，当然更严谨的需要通过测试集、验证集来验证在新数据集上的预测损失情况。</p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding=utf-8</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">np.random.seed(0)</span><br><span class="line"></span><br><span class="line">b = tf.Variable(0, dtype=tf.float32)</span><br><span class="line">w1 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line">w2 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line">w3 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line">w4 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">pie_size = np.array([-2, -1, 0, 1, 2], dtype=np.float32)</span><br><span class="line">pie_price = np.log((pie_size + 3) * 10)</span><br><span class="line">pie_price += np.random.normal(0, 0.3, [5])</span><br><span class="line"># print(pie_price)</span><br><span class="line"></span><br><span class="line">regularization_strength = 0.2</span><br><span class="line">regularization_result = regularization_strength \</span><br><span class="line">                        * (tf.abs(w1) + tf.abs(w2) + tf.abs(w3) + tf.abs(w4))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loss_for_underfitting():</span><br><span class="line">    target = b + w1 * pie_size</span><br><span class="line">    return tf.reduce_mean(tf.square(target - pie_price))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loss_for_ok():</span><br><span class="line">    target = b + w1 * pie_size + w2 * pie_size ** 2</span><br><span class="line">    return tf.reduce_mean(tf.square(target - pie_price))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loss_for_overfitting():</span><br><span class="line">    target = b + w1 * pie_size + w2 * pie_size ** 2 + w3 * pie_size ** 3 + w4 * pie_size ** 4</span><br><span class="line">    return tf.reduce_mean(tf.abs(target - pie_price))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loss_for_overfitting_scale():</span><br><span class="line">    # 2   4   8   16</span><br><span class="line">    target = b + w1 * pie_size + w2 * pie_size ** 2 / 2 + w3 * pie_size ** 3 / 4 + w4 * pie_size ** 4 / 8</span><br><span class="line">    return tf.reduce_mean(tf.abs(target - pie_price))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loss_for_overfitting_regular():</span><br><span class="line">    target = b + w1 * pie_size + w2 * pie_size ** 2 / 2 + w3 * pie_size ** 3 / 4 + w4 * pie_size ** 4 / 8</span><br><span class="line">    return tf.reduce_mean(tf.abs(target - pie_price)) + regularization_result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">method = 1</span><br><span class="line"></span><br><span class="line">if method == 0:</span><br><span class="line">    learn_rate = 0.01</span><br><span class="line">    loss = loss_for_underfitting()</span><br><span class="line">elif method == 1:</span><br><span class="line">    learn_rate = 0.0001</span><br><span class="line">    loss = loss_for_ok()</span><br><span class="line">elif method == 2:</span><br><span class="line">    learn_rate = 0.0001</span><br><span class="line">    loss = loss_for_overfitting()</span><br><span class="line">elif method == 10:</span><br><span class="line">    learn_rate = 0.01</span><br><span class="line">    loss = loss_for_overfitting_scale()</span><br><span class="line">else:</span><br><span class="line">    learn_rate = 0.001</span><br><span class="line">    loss = loss_for_overfitting_regular()</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learn_rate)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">step = 0</span><br><span class="line">last_loss1 = 0</span><br><span class="line">last_loss2 = 0</span><br><span class="line">while True:</span><br><span class="line">    step += 1</span><br><span class="line">    sess.run(train)</span><br><span class="line">    if step % 1_0000 == 0:</span><br><span class="line">        loss_value = sess.run(loss)</span><br><span class="line">        time_str = time.strftime(&quot;%H:%M:%S&quot;, time.localtime())  # %Y-%m-%d %H:%M:%S</span><br><span class="line">        print(f&apos;step=&#123;step&#125; time=&#123;time_str&#125; loss=&#123;loss_value:0.8f&#125;&apos;</span><br><span class="line">              + f&apos; bias=&#123;sess.run(b):0.4f&#125; w1=&#123;sess.run(w1):0.4f&#125; w2=&#123;sess.run(w2):0.4f&#125; w3=&#123;sess.run(w3):0.4f&#125; w4=&#123;sess.run(w4):0.4f&#125;&apos;)</span><br><span class="line">        if loss_value &lt; 0.01:</span><br><span class="line">            break</span><br><span class="line">        if last_loss1 == last_loss2 and last_loss2 == loss_value:</span><br><span class="line">            break</span><br><span class="line">        last_loss2 = last_loss1</span><br><span class="line">        last_loss1 = loss_value</span><br><span class="line">    if step &gt;= 50_0000:</span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line">_b = sess.run(b)</span><br><span class="line">_w1 = sess.run(w1)</span><br><span class="line">_w2 = sess.run(w2)</span><br><span class="line">_w3 = sess.run(w3)</span><br><span class="line">_w4 = sess.run(w4)</span><br><span class="line"></span><br><span class="line">print(f&apos;bias=&#123;_b&#125; w1=&#123;_w1&#125; w2=&#123;_w2&#125; w3=&#123;_w3&#125; w4=&#123;_w4&#125;&apos;)</span><br><span class="line"></span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line">best_fit = []</span><br><span class="line">x_array = np.linspace(pie_size[0] - 1, pie_size[len(pie_size) - 1] + 1, 10000)</span><br><span class="line">for x in x_array:</span><br><span class="line">    if method &lt; 10:</span><br><span class="line">        best_fit.append(_b + _w1 * x + _w2 * x ** 2 + _w3 * x ** 3 + _w4 * x ** 4)</span><br><span class="line">    else:</span><br><span class="line">        best_fit.append(_b + _w1 * x + _w2 * x ** 2 / 2 + _w3 * x ** 3 / 4 + _w4 * x ** 4 / 8)</span><br><span class="line"></span><br><span class="line"># print(f&apos;best_fit=&#123;best_fit&#125;&apos;)</span><br><span class="line"></span><br><span class="line">show_plt = 1</span><br><span class="line"></span><br><span class="line">if show_plt == 1:</span><br><span class="line">    plt.figure()</span><br><span class="line">plt.scatter(pie_size, pie_price, c=&apos;y&apos;, marker=&apos;o&apos;, label=&quot;pie&quot;)</span><br><span class="line">plt.plot(x_array, best_fit, color=&apos;b&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>源码地址：<a href="https://github.com/qianhk/FeiPython/blob/master/Python3Test/kaiLinear/kai_overfitting_regular3.py" target="_blank" rel="noopener">https://github.com/qianhk/FeiPython/blob/master/Python3Test/kaiLinear/kai_overfitting_regular3.py</a></p>
<h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p>吴恩达机器学习教程：<br><a href="http://study.163.com/course/introduction/1004570029.htm" target="_blank" rel="noopener">http://study.163.com/course/introduction/1004570029.htm</a></p>
<h2 id=""><a href="#" class="headerlink" title="　"></a>　</h2><p>本文首发于<a href="http://qianhk.com">钱凯凯的博客</a> : <a href="http://qianhk.com/2018/07/客户端码农学习ML-欠拟合_过拟合_正则化/">http://qianhk.com/2018/07/客户端码农学习ML-欠拟合_过拟合_正则化/</a></p>


      

    </section>
    
      <section class='ArticleMeta'>
          <div>
            发布于&nbsp;
            <time datetime="2018-07-15T09:30:12.000Z" itemprop="datePublished">
              2018-07-15
            </time>
          </div>
          
            <div>
              tags: 
  <li class="meta-text">
  { <a href="/tags/AI/">AI</a> }
  </li>

  <li class="meta-text">
  { <a href="/tags/拟合/">拟合</a> }
  </li>

  <li class="meta-text">
  { <a href="/tags/正则化/">正则化</a> }
  </li>


            </div>
          
      </section>
    
    
</article>

  <div id="comment-container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: '客户端码农学习ML-欠拟合_过拟合_正则化',
  owner: 'qianhk',
  repo: 'qianhk.github.io',
  oauth: {
    client_id: '158d8d047a5c66420ecb',
    client_secret: '5914732b33687719e97c518d1d0089a1a45ce3c0',
  },
})
gitment.render('comment-container')
</script>

</div>

            <footer>
    <div>© 2016 - qianhk </div>
    <div>
    Powered by Hexo
    </div>
</footer>

        </div>
    </div>
</div>
<script src="/js/pager/dist/singlepager.js"></script>
<script>
var sp = new Pager('data-pager-shell')

</script>
</body>
</html>