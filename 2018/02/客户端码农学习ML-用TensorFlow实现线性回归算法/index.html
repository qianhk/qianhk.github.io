<!DOCTYPE html>
<html lang="zh-cmn-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>客户端码农学习ML —— 用TensorFlow实现线性回归算法 | 钱凯凯</title>
  
  
  <!--link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css"-->
  <link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css">
  <link rel="stylesheet" href="/css/style.css">
  
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="Shell">
    <aside class='SideBar'>
    <section class='avatar' style="background-image: url()">
        <div class='av-pic' style="background-image: url(/assets/avatar.jpg)">
        </div>
    </section>
    <section class='menu'>
        <div>钱凯凯</div>
        
            <div>钱凯凯的博客</div>
        
        <ul>
          
            <a href="/" class="Btn">
              <li>Home</li>
            </a>  
          
            <a href="/archives/" class="Btn">
              <li>Archive</li>
            </a>  
          
        </ul>
    </section>
    <section class="media">
        
            
                <a href="https://github.com/qianhk">
                    <img src="/assets/github.svg" />
                </a>
            
        
    </section>
</aside>

    <div class="container">
        <div data-pager-shell>
            <div>
  <article class='ContentView'>
    <header class='PageTitle'>
        <h1>客户端码农学习ML —— 用TensorFlow实现线性回归算法</h1>
    </header>

    <section>
      <h1 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a>线性回归（Linear Regression）</h1><p>线性回归算法是机器学习、统计分析中重要的算法之一，也是常用的相对简单的算法。</p>
<p>微信小游戏跳一跳某辅助程序<a href="https://github.com/wangshub/wechat_jump_game" target="_blank" rel="noopener">wechat jump game</a>，之前要事先根据屏幕尺寸填写一个按压时间与弹跳距离的比例经验值并不断人为调整，后来可通过此算法拟合按压时间与弹跳距离了, <a href="https://github.com/wangshub/wechat_jump_game/pull/825" target="_blank" rel="noopener">Pull Request在此</a>。</p>
<p>给定由d个属性描述的点集X=(x<sub>1</sub>;x<sub>2</sub>;…;x<sub>d</sub>), 线性模型试图学得一个通过属性的线性组合来进行预测的函数，即&fnof;(x)=w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + … + w<sub>d</sub>x<sub>d</sub> + b，知道w和b后就能确定模型。</p>
<a id="more"></a>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>我们在高中数学中已经学过只有一个属性x求待定系数的算法，即最小二乘法，一系列离散点通过最小二乘法即可确定一条回归直线&fnof;(x)=kx+b，这种只有一个输入变量/特征值的问题也叫作单变量线性回归问题。</p>
<h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>不同的k值，也使得预测值与实际值的建模误差不同，方差是常用的一种损失函数，也叫代价函数（Cost Function），我们的目标就是找到可以使得方差最小的模型参数。</p>
<p>单变量线性回归的损失函数通图形化后常类似于一个抛物线，有一个最小值。</p>
<p><img src="/images/ai_cost_parabola.jpg" alt="ai_cost_parabola"></p>
<p>两个变量/特征的线性回归损失函数图形化后类似于一个碗，碗底就是最小值。</p>
<p><img src="/images/ai_cost_bowl.jpg" alt="ai_cost_bowl"></p>
<p>更多特征值的情况下，高维空间难以图形化，损失函数在不同区域有不同的极值，一般较难计算出最小值。</p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>我们通常采用梯度下降算法来求的这个最小值。先随机选择一个参数组合，计算损失函数，然后找下一个能让损失函数下降最多的新参数组合并同步更新，继续这么做直到找到一个局部最小值。不同的初始参数组合可能会找到不同的局部最小值。</p>
<p><img src="/images/ai_cost_gradient_descent1.jpg" alt="ai_cost_gradient_descent"></p>
<p>梯度下降算法公式：</p>
<p><img src="/images/ai_cost_gradient_descent2.png" alt="ai_cost_gradient_descent2"></p>
<p>其中α是学习率(learning rate)，α决定了沿着使得损失函数下降较大的方向迈出的步子有多大，值太小则收敛太慢，值太大则可能越过最小值，导致无法收敛或者无法找到合理的待定参数组合θ。</p>
<p><img src="/images/ai_cost_gradient_descent3.jpg" alt="ai_cost_gradient_descent3"></p>
<p>α右边是一个导数项，需要导数、偏导数的基础知识，简单来讲就是通过当前θ处的切线斜率来决定正确的方向，并配合学习率决定走多远。</p>
<p>我们现在用TensorFlow实现并体验下机器学习的思想。</p>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>首先通过numpy生成一些模拟数据并有意随机偏移点(x<sub>i</sub>,y<sub>i</sub>)，把生成的随机点当做数据集，并把数据集按照8:2的比例分成训练集与测试集，然后通过代码去读取训练集并更新欲求参数k、b，使得k、b越来越接近真实值，使得f(x<sub>i</sub>)≈y<sub>i</sub>，从而使得方差最小。</p>
<p>方差对应了欧几里得距离，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">from tensorflow.python.framework import ops</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">ops.reset_default_graph()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">data_amount = 101 # 数据数量</span><br><span class="line">batch_size = 25 # 批量大小</span><br><span class="line"></span><br><span class="line"># 造数据 y=Kx+3 (K=5)</span><br><span class="line">x_vals = np.linspace(20, 200, data_amount)</span><br><span class="line"></span><br><span class="line">y_vals = np.multiply(x_vals, 5)</span><br><span class="line">y_vals = np.add(y_vals, 3)</span><br><span class="line"></span><br><span class="line"># 生成一个N(0,15)的正态分布一维数组</span><br><span class="line">y_offset_vals = np.random.normal(0, 15, data_amount)</span><br><span class="line">y_vals = np.add(y_vals, y_offset_vals) # 为了有意使的y值有所偏差</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># 创建占位符</span><br><span class="line">x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)</span><br><span class="line">y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"># 构造K 就是要训练得到的值</span><br><span class="line">K = tf.Variable(tf.random_normal(mean=0, shape=[1, 1]))</span><br><span class="line"></span><br><span class="line">calcY = tf.add(tf.matmul(x_data, K), 3)</span><br><span class="line"></span><br><span class="line"># 真实值与模型估算的差值</span><br><span class="line">loss = tf.reduce_mean(tf.square(y_target - calcY))</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line">my_opt = tf.train.GradientDescentOptimizer(0.0000005)</span><br><span class="line">train_step = my_opt.minimize(loss) # 目的就是使损失值最小</span><br><span class="line"></span><br><span class="line">loss_vec = [] #保存每次迭代的损失值，为了图形化</span><br><span class="line"></span><br><span class="line">for i in range(1000):</span><br><span class="line">    rand_index = np.random.choice(data_amount, size=batch_size)</span><br><span class="line">    x = np.transpose([x_vals[rand_index]])</span><br><span class="line">    y = np.transpose([y_vals[rand_index]])</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x_data: x, y_target: y&#125;)</span><br><span class="line"></span><br><span class="line">    tmp_loss = sess.run(loss, feed_dict=&#123;x_data: x, y_target: y&#125;)</span><br><span class="line">    loss_vec.append(tmp_loss)</span><br><span class="line"># 每25的倍数输出往控制台输出当前训练数据供查看进度</span><br><span class="line">    if (i + 1) % 25 == 0:</span><br><span class="line">        print(&apos;Step #&apos; + str(i + 1) + &apos; K = &apos; + str(sess.run(K)))</span><br><span class="line">        print(&apos;Loss = &apos; + str(sess.run(loss, feed_dict=&#123;x_data: x, y_target: y&#125;)))</span><br><span class="line"></span><br><span class="line"># 当训练完成后k的值就是当前的得到的结果，可以通过sess.run(K)取得</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>学习框架会使用梯度下降法去寻找一个最优解, 使得方差最小。</p>
<p>学习率是个很重要的参数，如果过小，算法收敛耗时很长，如果过大，可能结果不收敛或者直接NAN无法得到结果。</p>
<h2 id="展示结果"><a href="#展示结果" class="headerlink" title="展示结果"></a>展示结果</h2><p>本次试验用到了numpy及matplot，后续再练习下这两个库的使用，加强下印象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">best_fit = []</span><br><span class="line">for i in x_vals:</span><br><span class="line">    best_fit.append(KValue * i + 3)</span><br><span class="line"></span><br><span class="line">plt.plot(x_vals, y_vals, &apos;o&apos;, label=&apos;Data&apos;)</span><br><span class="line">plt.plot(x_vals, best_fit, &apos;r-&apos;, label=&apos;Base fit line&apos;)</span><br><span class="line"># plt.plot(loss_vec, &apos;k-&apos;)</span><br><span class="line">plt.title(&apos;Batch Look Loss&apos;)</span><br><span class="line">plt.xlabel(&apos;Generation&apos;)</span><br><span class="line">plt.ylabel(&apos;Loss&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>图形化后有两张图，一张表示本次训练最后拟合直线，一张表示每次训练损失值的收敛情况，但结果不是唯一的。</p>
<p><img src="/images/ai_kai_line_only_mul_batch_line.png" alt="ai_kai_line_only_mul_batch_line"></p>
<p><img src="/images/ai_kai_line_only_mul_batch_loss.png" alt="ai_kai_line_only_mul_batch_loss"></p>
<p>可以看出，随着训练的进行，预测损失整体越来越小，改变学习率或者批量大小则会使训练损失收敛速度发生显著变化，甚至无法收敛，总体上批量数值越大效果越好。</p>
<h1 id="其他回归算法"><a href="#其他回归算法" class="headerlink" title="其他回归算法"></a>其他回归算法</h1><p>除了线性回归算法，还有其它好几种回归算法，后续陆续学习、补充。</p>
<h2 id="戴明回归算法"><a href="#戴明回归算法" class="headerlink" title="戴明回归算法"></a>戴明回归算法</h2><p>最小二乘线性回归算法是最小化到回归直线的竖直距离，只考虑y值，而戴明回归算法是最小化到回归直线垂直距离，同时考虑x值与y值。</p>
<p>具体算法修改下相减的损失函数即可，两者计算结果基本一致。</p>
<h2 id="lasso回归与岭回归算法"><a href="#lasso回归与岭回归算法" class="headerlink" title="lasso回归与岭回归算法"></a>lasso回归与岭回归算法</h2><p>主要是在公式中增加正则项来限制斜率，lasso回归增加L1正则项，岭回归增加L2正则项。</p>
<h2 id="弹性网络回归算法"><a href="#弹性网络回归算法" class="headerlink" title="弹性网络回归算法"></a>弹性网络回归算法</h2><p>综合lasso回归和岭回归的一种算法，在损失函数中同时增加L1和L2正则项。</p>
<h2 id="逻辑回归算法"><a href="#逻辑回归算法" class="headerlink" title="逻辑回归算法"></a>逻辑回归算法</h2><p>将线性回归转换成一个二值分类器，通过sigmoid函数将线性回归的输出缩放到0、1之间，判断目标是否属于某一类。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://studentdeng.github.io/blog/2014/07/28/machine-learning-tutorial/" target="_blank" rel="noopener">http://studentdeng.github.io/blog/2014/07/28/machine-learning-tutorial/</a></p>
<h1 id=""><a href="#" class="headerlink" title="　"></a>　</h1><p>本文首发于<a href="http://qianhk.com">钱凯凯的博客</a> : <a href="http://qianhk.com/2018/02/客户端码农学习ML-用TensorFlow实现线性回归算法/">http://qianhk.com/2018/02/客户端码农学习ML-用TensorFlow实现线性回归算法/</a></p>


      

    </section>
    
      <section class='ArticleMeta'>
          <div>
            发布于&nbsp;
            <time datetime="2018-02-27T14:26:50.000Z" itemprop="datePublished">
              2018-02-27
            </time>
          </div>
          
            <div>
              tags: 
  <li class="meta-text">
  { <a href="/tags/AI/">AI</a> }
  </li>

  <li class="meta-text">
  { <a href="/tags/Tensorflow/">Tensorflow</a> }
  </li>

  <li class="meta-text">
  { <a href="/tags/Linear-Regression/">Linear Regression</a> }
  </li>


            </div>
          
      </section>
    
    
</article>

  <div id="comment-container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: '客户端码农学习ML-用TensorFlow实现线性回归算法',
  owner: 'qianhk',
  repo: 'qianhk.github.io',
  oauth: {
    client_id: '158d8d047a5c66420ecb',
    client_secret: '5914732b33687719e97c518d1d0089a1a45ce3c0',
  },
})
gitment.render('comment-container')
</script>

</div>

            <footer>
    <div>© 2016 - qianhk </div>
    <div>
    Powered by Hexo
    </div>
</footer>

        </div>
    </div>
</div>
<script src="/js/pager/dist/singlepager.js"></script>
<script>
var sp = new Pager('data-pager-shell')

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>